{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81d2d55-f8d4-4825-8e83-b7e30935ec3e",
   "metadata": {},
   "source": [
    "Importing proper libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f69e09-1c04-457d-83fb-c258468aa4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql.functions import col, count, sum as _sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f7b44-e912-4524-8807-118ce44a5ff3",
   "metadata": {},
   "source": [
    "Initializing Global Variables, Including MYSQL, MONGODB, DIRECTORY FOR SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce540a9-7b3a-47d1-871c-e6231aab7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventure_dw2\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"WreckedRatUVA123!\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"dev\",\n",
    "    \"password\" : \"abc123!\",\n",
    "    \"cluster_name\" : \"Cluster0\",\n",
    "    \"cluster_subnet\" : \"zyvokl5\",\n",
    "    \"cluster_location\" : \"atlas\", # \"local\"\n",
    "    \"collection\" : \"AdventureWorks\",\n",
    "    \"null_column_threshold\" : 0.5,\n",
    "    \"db_name\" : \"AdventureWorks\"\n",
    "}\n",
    "    \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.getcwd()\n",
    "batch_dir = os.path.join(base_dir, 'batch')\n",
    "stream_dir = os.path.join(base_dir, 'streaming')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"adventureworks_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "sales_output_bronze = os.path.join(database_dir, 'fact_sales', 'bronze')\n",
    "sales_output_silver = os.path.join(database_dir, 'fact_sales', 'silver')\n",
    "sales_output_gold = os.path.join(database_dir, 'fact_sales', 'gold')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc72b3-62cf-400b-940a-a7823282dd8f",
   "metadata": {},
   "source": [
    "Global Functions to help simiplify process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66c87da-56a8-4ce8-8f90-fbf25b6ff466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7662d9-084e-47ee-b3c4-8972203c7cd9",
   "metadata": {},
   "source": [
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383ca223-3599-495e-8cf2-27be15920877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'C:\\\\Users\\\\devpp\\\\Desktop\\\\project1\\\\spark-warehouse\\\\adventureworks_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51ceab-84b3-4c7b-a4fa-3de8b2fda73b",
   "metadata": {},
   "source": [
    "Creating a new Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d518f8-a1d2-4ab2-8534-2672bab2c539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DevXPS:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Northwind Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1add9482030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9eb76-a04f-4c65-a829-fec537e9cebf",
   "metadata": {},
   "source": [
    "Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6e6860-023d-49af-afde-587663faddcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d5e2b-113c-4310-bcfb-c7a99b0391f9",
   "metadata": {},
   "source": [
    "#### Lets populate the Dimensions now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127e902-dd3d-47c4-933b-07090e0da87e",
   "metadata": {},
   "source": [
    "Lets see If the dim_customer is in the batch directory. That we did in Project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df276fd-a55c-40e3-81a0-1c48456ff77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dim_customer.csv</td>\n",
       "      <td>1438010</td>\n",
       "      <td>2025-12-19 15:33:13.960261106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name     size             modification_time\n",
       "0  dim_customer.csv  1438010 2025-12-19 15:33:13.960261106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40151651-68ab-48be-9297-5cee1d8d4b09",
   "metadata": {},
   "source": [
    "Lets populate the dim_customer_table and save it as dim_customers in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0502ebe-038a-4383-a744-15e043a4d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devpp\\Desktop\\project1\\batch\\dim_customer.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "customers_csv = os.path.join(batch_dir, 'dim_customer.csv')\n",
    "print(customers_csv)\n",
    "df_dim_customers = spark.read.format('csv').options(header='true', inferSchema='true').load(customers_csv)\n",
    "window_spec = Window.orderBy(\"CustomerID\")\n",
    "df_dim_customers = df_dim_customers.withColumn(\"customer_key\", F.row_number().over(window_spec))\n",
    "df_dim_customers.toPandas().head(2)\n",
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e55c50-c8d6-4d08-9c95-5b3114c864af",
   "metadata": {},
   "source": [
    "Lets test the table to make sure it displays properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d14e0a03-f1e0-4dee-b4f0-3298c6dd7f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          CustomerID|                 int|   NULL|\n",
      "|       AccountNumber|              string|   NULL|\n",
      "|        CustomerType|              string|   NULL|\n",
      "| SalesTerritoryGroup|              string|   NULL|\n",
      "|      SalesTerritory|              string|   NULL|\n",
      "|        AddressLine1|              string|   NULL|\n",
      "|                City|              string|   NULL|\n",
      "|          PostalCode|              string|   NULL|\n",
      "|        customer_key|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Fri Dec 19 17:19:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/de...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>SalesTerritoryGroup</th>\n",
       "      <th>SalesTerritory</th>\n",
       "      <th>AddressLine1</th>\n",
       "      <th>City</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>customer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>S</td>\n",
       "      <td>North America</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>2251 Elliot Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>98104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>S</td>\n",
       "      <td>North America</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>7943 Walnut Ave</td>\n",
       "      <td>Renton</td>\n",
       "      <td>98055</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID AccountNumber CustomerType SalesTerritoryGroup SalesTerritory  \\\n",
       "0           1    AW00000001            S       North America      Northwest   \n",
       "1           2    AW00000002            S       North America      Northwest   \n",
       "\n",
       "         AddressLine1     City PostalCode  customer_key  \n",
       "0  2251 Elliot Avenue  Seattle      98104             1  \n",
       "1     7943 Walnut Ave   Renton      98055             2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a3572-4522-4574-b175-f876ace069cf",
   "metadata": {},
   "source": [
    "Lets insert the product.json file into Mongo Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948757fc-537e-455f-97a2-dee5d466bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"Products\" : \"dim_product.json\"}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], os.getcwd(), json_files) \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d58b10-464a-4ceb-b1cc-ebbaa5324b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_args[\"collection\"] = \"Products\"\n",
    "df_dim_Products = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_Products.toPandas().head(2)\n",
    "window_spec = Window.orderBy(\"ProductID\")\n",
    "df_dim_Products = df_dim_Products.withColumn(\"product_key\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fde6ec-20c0-4bb7-a88c-be33a1319cb2",
   "metadata": {},
   "source": [
    "Lets reorder the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8585f32-f273-46ff-aae1-cdff6820bde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>SellStartDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber MakeFlag  \\\n",
       "0            1          1  Adjustable Race       AR-5381  b'\\x00'   \n",
       "1            2          2     Bearing Ball       BA-8327  b'\\x00'   \n",
       "\n",
       "  FinishedGoodsFlag  SafetyStockLevel  ReorderPoint  DaysToManufacture  \\\n",
       "0           b'\\x00'              1000           750                  0   \n",
       "1           b'\\x00'              1000           750                  0   \n",
       "\n",
       "  SellStartDate  \n",
       "0    1998-06-01  \n",
       "1    1998-06-01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_Products = df_dim_Products[[\n",
    "    'product_key',\n",
    "    'ProductID',\n",
    "    'Name',\n",
    "    'ProductNumber',\n",
    "    'MakeFlag',\n",
    "    'FinishedGoodsFlag',\n",
    "    'SafetyStockLevel',\n",
    "    'ReorderPoint',\n",
    "    'DaysToManufacture',\n",
    "    'SellStartDate',\n",
    "]]\n",
    "df_dim_Products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41461e-fc65-4674-af61-09d833c999b9",
   "metadata": {},
   "source": [
    "Lets save the dim_Products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d4d249-226e-433a-8228-0b331e849e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         product_key|                 int|   NULL|\n",
      "|           ProductID|                 int|   NULL|\n",
      "|                Name|              string|   NULL|\n",
      "|       ProductNumber|              string|   NULL|\n",
      "|            MakeFlag|              string|   NULL|\n",
      "|   FinishedGoodsFlag|              string|   NULL|\n",
      "|    SafetyStockLevel|                 int|   NULL|\n",
      "|        ReorderPoint|                 int|   NULL|\n",
      "|   DaysToManufacture|                 int|   NULL|\n",
      "|       SellStartDate|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|        dim_products|       |\n",
      "|        Created Time|Fri Dec 19 17:19:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>SellStartDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>b'\\x00'</td>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber MakeFlag  \\\n",
       "0            1          1  Adjustable Race       AR-5381  b'\\x00'   \n",
       "1            2          2     Bearing Ball       BA-8327  b'\\x00'   \n",
       "\n",
       "  FinishedGoodsFlag  SafetyStockLevel  ReorderPoint  DaysToManufacture  \\\n",
       "0           b'\\x00'              1000           750                  0   \n",
       "1           b'\\x00'              1000           750                  0   \n",
       "\n",
       "  SellStartDate  \n",
       "0    1998-06-01  \n",
       "1    1998-06-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_Products.write.saveAsTable(f\"{dest_database}.dim_Products\", mode=\"overwrite\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_Products;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_Products LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4dff2-66ef-4f6e-afd3-0e05a6fe2bca",
   "metadata": {},
   "source": [
    "Lets populate from MySQL Database.\n",
    "We need the Date Dimension Table and the Employee dimension Table.\n",
    "\n",
    "Here is the Date Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897081ff-b6ab-40a9-b6be-b3e7a70f4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|           full_date|timestamp|   NULL|\n",
      "|            date_key|   bigint|   NULL|\n",
      "|           date_name|   string|   NULL|\n",
      "|        date_name_us|   string|   NULL|\n",
      "|        date_name_eu|   string|   NULL|\n",
      "|         day_of_week|      int|   NULL|\n",
      "|    day_name_of_week|   string|   NULL|\n",
      "|        day_of_month|      int|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend|   string|   NULL|\n",
      "|        week_of_year|   bigint|   NULL|\n",
      "|          month_name|   string|   NULL|\n",
      "|       month_of_year|      int|   NULL|\n",
      "|is_last_day_of_month|   string|   NULL|\n",
      "|    calendar_quarter|      int|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month|   string|   NULL|\n",
      "|   calendar_year_qtr|   string|   NULL|\n",
      "|fiscal_month_of_year|      int|   NULL|\n",
      "|      fiscal_quarter|      int|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_key</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>20000101</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>20000102</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   full_date  date_key   date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0 2000-01-01  20000101  2000/01/01   01/01/2000   01/01/2000            6   \n",
       "1 2000-01-02  20000102  2000/01/02   01/02/2000   02/01/2000            7   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0         Saturday             1            1         Weekend  ...   \n",
       "1           Sunday             2            2         Weekend  ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000             2000-01   \n",
       "1                     N                1           2000             2000-01   \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0             2000Q1                     7              3        2000   \n",
       "1             2000Q1                     7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0            2000-07           2000Q3  \n",
       "1            2000-07           2000Q3  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)\n",
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b7542-1384-4863-9c2b-af31bac74565",
   "metadata": {},
   "source": [
    "Here is the Employee Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89066849-2a24-4b47-97c5-17aa3b32af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_employee = f\"SELECT * FROM {mysql_args['db_name']}.dim_employee\"\n",
    "df_dim_employee = get_mysql_dataframe(spark, sql_employee, **mysql_args)\n",
    "window_spec = Window.orderBy(\"EmployeeID\")\n",
    "df_dim_employee = df_dim_employee.withColumn(\"employee_key\", F.row_number().over(window_spec))\n",
    "df_dim_employee.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87bf27d3-ae36-43ee-8a17-63cae5be470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          EmployeeID|              bigint|   NULL|\n",
      "|               Title|              string|   NULL|\n",
      "|           BirthDate|           timestamp|   NULL|\n",
      "|            HireDate|           timestamp|   NULL|\n",
      "|              Gender|              string|   NULL|\n",
      "|       MaritalStatus|              string|   NULL|\n",
      "|      DepartmentName|              string|   NULL|\n",
      "|           ManagerID|              double|   NULL|\n",
      "|       VacationHours|              bigint|   NULL|\n",
      "|      SickLeaveHours|              bigint|   NULL|\n",
      "|        employee_key|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_employees|       |\n",
      "|        Created Time|Fri Dec 19 17:19:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Title</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>DepartmentName</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>VacationHours</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>employee_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>1972-05-15</td>\n",
       "      <td>1996-07-31</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>Production</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>1977-06-03</td>\n",
       "      <td>1997-02-26</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>6.0</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID                         Title  BirthDate   HireDate Gender  \\\n",
       "0           1  Production Technician - WC60 1972-05-15 1996-07-31      M   \n",
       "1           2           Marketing Assistant 1977-06-03 1997-02-26      M   \n",
       "\n",
       "  MaritalStatus DepartmentName  ManagerID  VacationHours  SickLeaveHours  \\\n",
       "0             M     Production       16.0             21              30   \n",
       "1             S      Marketing        6.0             42              41   \n",
       "\n",
       "   employee_key  \n",
       "0             1  \n",
       "1             2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5b33c-843e-4079-a5be-049f21569e4e",
   "metadata": {},
   "source": [
    "We have now gotten all of the Dimensions set up, \n",
    "\n",
    "We now need to create a facts table that Bronze - Silver - Gold Layer Facts Sales Table\n",
    "\n",
    "Bronze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ad42717-7ee4-473a-9041-185f10db709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", sales_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(stream_dir)\n",
    ")\n",
    "\n",
    "df_sales_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77b1a7-fd93-4364-a7d2-45012a2fa330",
   "metadata": {},
   "source": [
    "Write the streaming data to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda2aa36-cceb-446f-9e0a-accebd3a7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_checkpoint_bronze = os.path.join(sales_output_bronze, '_checkpoint')\n",
    "\n",
    "sales_bronze_query = (\n",
    "    df_sales_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"sales_bronze\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", sales_checkpoint_bronze)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(sales_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55347bc0-b9fd-490d-be4e-6a8156f477e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 25044b80-5f73-47fa-80fb-1992b3338c24\n",
      "Query Name: sales_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {sales_bronze_query.id}\")\n",
    "print(f\"Query Name: {sales_bronze_query.name}\")\n",
    "print(f\"Query Status: {sales_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e39cf1b-55e9-4ce2-b0b1-fd83ad0b9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ba8d7-0301-465b-9ac7-3c07c5093d8b",
   "metadata": {},
   "source": [
    "Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fadc680-da5d-4170-9b33-4b7fcfb43b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))\n",
    "df_dim_shipped_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"), col(\"full_date\").alias(\"shipped_full_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3be3ac3-e15e-44b9-ba2a-fd0595321715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_stream = spark.readStream.format(\"parquet\")    .option(\"maxFilesPerTrigger\", 5) \\\n",
    ".load(sales_output_bronze) \n",
    "\n",
    "df_sales_silver = (\n",
    "    df_sales_stream\n",
    "    .join(df_dim_customers, on=\"CustomerID\")\n",
    "    .join(df_dim_Products, on=\"ProductID\")\n",
    "    .join(df_dim_employee, on=\"EmployeeID\")\n",
    "    .join(df_dim_order_date, df_dim_order_date.order_full_date.cast(DateType()) == col(\"order_full_date\").cast(DateType()), \"inner\") \\\n",
    "    .join(df_dim_shipped_date, df_dim_shipped_date.shipped_full_date.cast(DateType()) == col(\"shipped_full_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .select(\n",
    "        col(\"salesorderid\").cast(LongType()),\n",
    "        df_dim_customers.customer_key.cast(LongType()), \\\n",
    "        df_dim_employee.employee_key.cast(LongType()), \\\n",
    "        df_dim_Products.product_key.cast(LongType()), \\\n",
    "        df_dim_order_date.order_date_key.cast(LongType()), \\\n",
    "        df_dim_shipped_date.shipped_date_key.cast(LongType()), \\\n",
    "        col(\"orderqty\"),\n",
    "        col(\"unitprice\"),\n",
    "        col(\"linetotal\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd340833-2e01-4bfe-a720-0fae459ea67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f51e6463-f14e-4bad-8c55-10bceafcfbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salesorderid: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- employee_key: long (nullable = false)\n",
      " |-- product_key: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- orderqty: long (nullable = true)\n",
      " |-- unitprice: double (nullable = true)\n",
      " |-- linetotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a53f1084-bf89-4adc-b9a2-e826dbb6faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 232b04e6-5d09-4c29-a935-6537d3dc8d5f\n",
      "Query Name: sales_silver\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "sales_checkpoint_silver = os.path.join(sales_output_silver, '_checkpoint')\n",
    "\n",
    "sales_silver_query = (\n",
    "    df_sales_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sales_silver\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .option(\"checkpointLocation\", sales_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(sales_output_silver))\n",
    "\n",
    "print(f\"Query ID: {sales_silver_query.id}\")\n",
    "print(f\"Query Name: {sales_silver_query.name}\")\n",
    "print(f\"Query Status: {sales_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c8f96fc-ebea-48a1-89a9-f0c5c01bcd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sales_silver_query.awaitTermination()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac42935-4fb7-4bcf-8f22-87815450f486",
   "metadata": {},
   "source": [
    "Gold Layer\n",
    "\n",
    "Lets make a table that shows how much sales each manager is responsible for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3089cc45-39b8-4924-97dc-cf2c1087dc12",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name employee_sales as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 41\u001b[0m\n\u001b[0;32m      8\u001b[0m df_sales_gold \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     df_sales_silver\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m df_employee_sales_monthly \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     23\u001b[0m     df_sales_gold\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalendar_year\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_of_year\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManagerID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m employee_sales_gold_query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     37\u001b[0m     df_employee_sales_monthly\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memployee_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     42\u001b[0m )\n",
      "File \u001b[1;32mC:\\spark-3.5.4-bin-hadoop3\\python\\pyspark\\sql\\streaming\\readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart())\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[1;32mC:\\spark-3.5.4-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark-3.5.4-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Cannot start query with name employee_sales as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "df_dim_order_date = df_dim_date.select(\n",
    "    col(\"date_key\").alias(\"order_date_key\"),\n",
    "    col(\"calendar_year\"),\n",
    "    col(\"month_of_year\"),\n",
    "    col(\"month_name\")\n",
    ")\n",
    "\n",
    "df_sales_gold = (\n",
    "    df_sales_silver.alias(\"f\")\n",
    "    .join(\n",
    "        df_dim_order_date.alias(\"d\"),\n",
    "        col(\"f.order_date_key\") == col(\"d.order_date_key\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_employee.alias(\"e\"),\n",
    "        col(\"f.employee_key\") == col(\"e.employee_key\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_employee_sales_monthly = (\n",
    "    df_sales_gold\n",
    "    .groupBy(\n",
    "        col(\"d.calendar_year\"),\n",
    "        col(\"d.month_of_year\"),\n",
    "        col(\"e.ManagerID\")\n",
    "    )\n",
    "    .agg(\n",
    "        count(\"f.salesorderid\").alias(\"monthly_sales_count\"),\n",
    "        _sum(\"f.linetotal\").alias(\"monthly_revenue\")\n",
    "    )\n",
    "    .orderBy(\"calendar_year\",\"month_of_year\", \"ManagerID\")\n",
    ")\n",
    "\n",
    "employee_sales_gold_query = (\n",
    "    df_employee_sales_monthly.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"employee_sales\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65d0b9eb-079d-44d1-b202-abc30b515263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 18 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(employee_sales_gold_query, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9e7b994-5dd0-4a1d-b82a-b288a804a84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- calendar_year: integer (nullable = true)\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- ManagerID: double (nullable = true)\n",
      " |-- monthly_sales_count: long (nullable = false)\n",
      " |-- monthly_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee_sales = spark.sql(\"SELECT * FROM employee_sales\")\n",
    "df_employee_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "204c1239-b75d-4a1b-86ff-fd354a5dde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee_sales_final = df_employee_sales \\\n",
    ".select(\n",
    "        col(\"ManagerID\"),\n",
    "        col(\"calendar_year\").alias(\"year\"),\n",
    "        col(\"month_of_year\").alias(\"month\"),\n",
    "        col(\"monthly_revenue\"),\n",
    "        col(\"monthly_sales_count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36ea5703-c0ea-4a1f-8b2d-f222d2652503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>monthly_revenue</th>\n",
       "      <th>monthly_sales_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.522292e+08</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.845708e+05</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.359564e+08</td>\n",
       "      <td>116522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>284.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>6.404049e+05</td>\n",
       "      <td>116522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.522292e+08</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>284.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>6</td>\n",
       "      <td>6.624878e+05</td>\n",
       "      <td>120540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>268.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>2.522292e+08</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>284.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>6.845708e+05</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>268.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>2.522292e+08</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>284.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>6.845708e+05</td>\n",
       "      <td>124558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ManagerID  year  month  monthly_revenue  monthly_sales_count\n",
       "0        268.0  2000      1     2.522292e+08               124558\n",
       "1        284.0  2000      1     6.845708e+05               124558\n",
       "2        268.0  2000      2     2.359564e+08               116522\n",
       "3        284.0  2000      2     6.404049e+05               116522\n",
       "4        268.0  2000      3     2.522292e+08               124558\n",
       "..         ...   ...    ...              ...                  ...\n",
       "259      284.0  2007      6     6.624878e+05               120540\n",
       "260      268.0  2007      7     2.522292e+08               124558\n",
       "261      284.0  2007      7     6.845708e+05               124558\n",
       "262      268.0  2007      8     2.522292e+08               124558\n",
       "263      284.0  2007      8     6.845708e+05               124558\n",
       "\n",
       "[264 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_employee_sales_final.write.saveAsTable(f\"{dest_database}.employee_sales_final\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.employee_sales_final\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42394ed-8901-4aba-86b2-3c799670db7c",
   "metadata": {},
   "source": [
    "Stop The Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03ecf3ab-4b4a-4bb2-813b-c2fe3040b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdbada-f93c-4176-84d3-b92933ad5c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pysparkenv)",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
